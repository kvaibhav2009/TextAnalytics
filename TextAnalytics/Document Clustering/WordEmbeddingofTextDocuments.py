# coding: utf-8
from textblob import TextBlob
import os
import codecs
from gensim.models import Phrases
import sys
reload(sys)
import os
from tika import parser
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize
import pickle
import pandas as pd
from gensim.models import Word2Vec
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

kmeans = KMeans()


sys.setdefaultencoding('utf8')

text='POC for use case “CDAT” is planned using below open source packages in Python (version 2.7) Gensim(version 3.2.0) (https://pypi.org/project/gensim/) Apache Tika Python library (version 1.16) Why Doc2vec is chosen: Doc2vec is an unsupervised algorithm to generate vectors for sentence/paragraphs/documents The algorithm is an adaptation of word2vec which can generate vectors for words. Doc2Vec computes a feature vector for every document in the corpus. The vectors generated by doc2vec can be used for tasks like finding similarity between sentences/paragraphs/documents. Doc2Vec’s learning strategy exploits the idea that the prediction of neighboring words for a given word strongly relies on the document also Using ontology, Doc2vec results can improve the relevance of search results We are using English Wikipedia Doc2vec model (enwiki_dbow) which pretrained on Wikipedia dataset. Why Apache Tika Python library (version 1.16) : Apache Tika is a library that is used for document type detection and content extraction from various file formats(.doc,.xlsx,.pptx). Internally, Tika uses existing various document parsers and document type detection techniques to detect and extract data.'


blob=TextBlob(text)




def filteringText(sentence,stem=False,lemmatize=True,clean_special_chars_numbers=True,remove_stopwords=True,stops=set(stopwords.words("english"))):
    filteredText=list()
    if sentence.startswith('==') == False:
        sentence_text = sentence

        # Optionally remove non-letters (true by default)
        if clean_special_chars_numbers:
            sentence_text = re.sub("[^a-zA-Z]", " ", sentence_text)

        # Convert words to lower case and split them
        words = sentence_text.lower().split()

        # Optional stemmer
        if stem:
            stemmer = PorterStemmer()
            words = [stemmer.stem(w) for w in words]

        if lemmatize:
            lemmatizer = WordNetLemmatizer()
            words = [lemmatizer.lemmatize(w) for w in words]

        # Optionally remove stop words (false by default)
        if remove_stopwords:
            words = [w for w in words if not w in stops]

        filteredText.append(words)
    return filteredText


ppt_filename="SOW 96 Extension - AI sample 1.pptx"

raw=parser.from_file(ppt_filename)
text=raw['content']

blob=TextBlob(text)

with open('ConvertBigram.pickle', 'rb') as handle:
    ConvertBigram = pickle.load(handle)

df=pd.DataFrame()





model=Word2Vec.load("Model/SoftModel_bigram_w2v")

def word_vectorize(query):
    query=query[0]
    count=0
    dim=int(model.wv.syn0.shape[1])
    vec = np.zeros(dim).reshape((1, dim))
    for word in query:
        try:
            vec+=model.wv[word]
            count+=1
        except KeyError:
            continue

    if count != 0:
        vec /= count

    print("Known Words %d/%d"% (count,query.__len__()))

    return vec



WordEmbedding=list()
tokendf=pd.DataFrame()
vectordf=pd.DataFrame()

for sentence in blob.sentences:
    print sentence
    sent=filteringText(str(sentence))
    print(sent)
    print("--------------------")
    print ConvertBigram[sent]
    tokendf=tokendf.append([list(ConvertBigram[sent])],ignore_index=True)
    #word_vectorize(ConvertBigram[sent])
    WordEmbedding.append(list(word_vectorize(list(ConvertBigram[sent]))))
    x=pd.DataFrame(np.array(word_vectorize(list(ConvertBigram[sent]))))
    vectordf=vectordf.append(x)

#model=Word2Vec.load("Model/SoftModel_bigram_w2v")


def  eliminate_number(sentence):
    sent=list()
    for word in sentence.split():
        try:
            int(word)
        except ValueError:
            sent.append(word)
            continue
    sentence=str(" ".join(sent))
    return sentence


df.to_excel("FilteredTextBigram.xlsx")
vectordf.to_excel("Documentvectors.xlsx")
kmeans = KMeans(5)

kmeans.fit(vectordf.values)

from sklearn.decomposition import PCA
pca = PCA(n_components=2).fit(vectordf.values)
pca_2d = pca.transform(vectordf.values)

plt.scatter(pca_2d[:, 0], pca_2d[:, 1],kmeans.cluster_centers_)

Xlabel=range(1,26)

for label, x, y in zip(Xlabel, pca_2d[:, 0], pca_2d[:, 1]):
    plt.annotate(
        label,
        xy=(x, y))

plt.savefig("DocumentClustering.png")


for label, x, y in zip(Xlabel, pca_2d[:, 0], pca_2d[:, 1]):
    plt.annotate(
        label,
        xy=(x, y), xytext=(-20, 20),
        textcoords='offset points', ha='right', va='bottom',
        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),
        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))

dic=dict(zip(Xlabel,kmeans.labels_))

for x in dic:
  if dic[x]==0:
    print x

print("\n----\n")
for cluster in set(dic.values()):
    print("Cluster %d"% cluster)
    for x in dic:
        if dic[x] == cluster:
            print x
    print("\n----\n")







